#!/usr/bin/env python3
"""
EUREKA Massive Question Bank Generator
Generates 3000+ questions per exam with proper IRT parameters and topic distribution
"""

import json
import random
import uuid
from datetime import datetime
from typing import Dict, List, Any, Tuple
import numpy as np

class MassiveQBankGenerator:
    """Generate thousands of questions with IRT parameters for multiple exams"""
    
    def __init__(self):
        self.exam_configs = self._setup_exam_configs()
        self.generated_questions = {exam: [] for exam in ['SAT', 'GRE', 'LSAT', 'MCAT']}
        self.stats = {exam: {'total': 0, 'by_section': {}, 'by_difficulty': {}} 
                     for exam in ['SAT', 'GRE', 'LSAT', 'MCAT']}
    
    def _setup_exam_configs(self) -> Dict:
        """Setup exam-specific configurations including topics and sections"""
        return {
            'SAT': {
                'sections': {
                    'Math': {
                        'topics': [
                            ['Algebra', ['Linear Equations', 'Systems', 'Inequalities', 'Absolute Value', 'Functions']],
                            ['Advanced Math', ['Quadratics', 'Polynomials', 'Exponentials', 'Radicals', 'Rational Expressions']],
                            ['Problem Solving', ['Word Problems', 'Data Analysis', 'Rates', 'Percentages', 'Ratios']],
                            ['Geometry', ['Triangles', 'Circles', 'Angles', 'Coordinate Geometry', 'Trigonometry']],
                            ['Statistics', ['Mean/Median/Mode', 'Standard Deviation', 'Probability', 'Data Interpretation']]
                        ],
                        'weight': 0.5
                    },
                    'Reading': {
                        'topics': [
                            ['Evidence-Based', ['Main Idea', 'Supporting Details', 'Inference', 'Author Purpose']],
                            ['Vocabulary', ['Context Clues', 'Word Choice', 'Tone', 'Connotation']],
                            ['Analysis', ['Structure', 'Rhetoric', 'Synthesis', 'Comparison']],
                            ['Command of Evidence', ['Citing Evidence', 'Data Interpretation', 'Paired Passages']]
                        ],
                        'weight': 0.35
                    },
                    'Writing': {
                        'topics': [
                            ['Grammar', ['Subject-Verb Agreement', 'Pronoun Usage', 'Verb Tense', 'Modifiers']],
                            ['Usage', ['Word Choice', 'Concision', 'Style', 'Tone']],
                            ['Punctuation', ['Commas', 'Semicolons', 'Apostrophes', 'Dashes']],
                            ['Organization', ['Transitions', 'Logical Sequence', 'Introduction/Conclusion']]
                        ],
                        'weight': 0.15
                    }
                },
                'num_choices': 4,
                'time_per_q': 75,
                'guessing_param': 0.25
            },
            'GRE': {
                'sections': {
                    'Quant': {
                        'topics': [
                            ['Arithmetic', ['Properties', 'Operations', 'Ratios', 'Percentages', 'Sequences']],
                            ['Algebra', ['Linear Equations', 'Quadratic Equations', 'Inequalities', 'Functions', 'Coordinate Geometry']],
                            ['Geometry', ['Lines and Angles', 'Triangles', 'Quadrilaterals', 'Circles', '3D Figures']],
                            ['Data Analysis', ['Statistics', 'Probability', 'Permutations', 'Combinations', 'Data Interpretation']],
                            ['Number Properties', ['Integers', 'Prime Numbers', 'Divisibility', 'Remainders', 'Patterns']]
                        ],
                        'weight': 0.5
                    },
                    'Verbal': {
                        'topics': [
                            ['Text Completion', ['Single Blank', 'Double Blank', 'Triple Blank', 'Context Clues']],
                            ['Sentence Equivalence', ['Synonyms', 'Contextual Meaning', 'Precision', 'Nuance']],
                            ['Reading Comprehension', ['Main Idea', 'Supporting Ideas', 'Inference', 'Structure']],
                            ['Critical Reasoning', ['Argument Analysis', 'Assumptions', 'Strengthen/Weaken', 'Paradox']]
                        ],
                        'weight': 0.5
                    }
                },
                'num_choices': 5,
                'time_per_q': 90,
                'guessing_param': 0.20
            },
            'LSAT': {
                'sections': {
                    'Logical Reasoning': {
                        'topics': [
                            ['Arguments', ['Main Conclusion', 'Role of Statement', 'Method of Reasoning', 'Parallel Reasoning']],
                            ['Assumptions', ['Necessary Assumption', 'Sufficient Assumption', 'Flaw in Reasoning']],
                            ['Evidence', ['Strengthen', 'Weaken', 'Evaluate', 'Resolve Paradox']],
                            ['Formal Logic', ['Conditional Reasoning', 'Contrapositive', 'Quantifiers', 'Set Relationships']],
                            ['Causal Reasoning', ['Cause and Effect', 'Correlation vs Causation', 'Alternative Causes']]
                        ],
                        'weight': 0.5
                    },
                    'Analytical Reasoning': {
                        'topics': [
                            ['Ordering', ['Linear', 'Circular', 'Multi-Layer', 'Relative Positioning']],
                            ['Grouping', ['Selection', 'Distribution', 'Matching', 'Hybrid']],
                            ['Assignment', ['Scheduling', 'Mapping', 'Networks', 'Processes']],
                            ['Conditional', ['If-Then Rules', 'Chains', 'Biconditionals', 'Rule Substitution']]
                        ],
                        'weight': 0.25
                    },
                    'Reading Comprehension': {
                        'topics': [
                            ['Law Passages', ['Legal Principles', 'Case Analysis', 'Jurisprudence', 'Legal History']],
                            ['Science Passages', ['Hypotheses', 'Experiments', 'Theories', 'Scientific Method']],
                            ['Humanities', ['Philosophy', 'Literature', 'Art Theory', 'Cultural Studies']],
                            ['Social Sciences', ['Economics', 'Sociology', 'Psychology', 'Political Science']]
                        ],
                        'weight': 0.25
                    }
                },
                'num_choices': 5,
                'time_per_q': 85,
                'guessing_param': 0.20
            },
            'MCAT': {
                'sections': {
                    'Chem/Phys': {
                        'topics': [
                            ['General Chemistry', ['Atomic Structure', 'Periodic Trends', 'Bonding', 'Stoichiometry', 'Solutions']],
                            ['Physics', ['Mechanics', 'Waves', 'Electricity', 'Optics', 'Thermodynamics']],
                            ['Organic Chemistry', ['Nomenclature', 'Reactions', 'Mechanisms', 'Spectroscopy', 'Lab Techniques']],
                            ['Biochemistry', ['Amino Acids', 'Proteins', 'Enzymes', 'Metabolism', 'Nucleic Acids']]
                        ],
                        'weight': 0.25
                    },
                    'CARS': {
                        'topics': [
                            ['Foundations', ['Main Idea', 'Tone', 'Purpose', 'Structure']],
                            ['Reasoning Within', ['Function', 'Definition', 'Analogy', 'Example']],
                            ['Reasoning Beyond', ['Application', 'Implication', 'Extrapolation', 'Challenge']],
                            ['Critical Analysis', ['Assumptions', 'Evidence', 'Bias', 'Perspective']]
                        ],
                        'weight': 0.25
                    },
                    'Bio/Biochem': {
                        'topics': [
                            ['Biology', ['Cell Biology', 'Genetics', 'Evolution', 'Ecology', 'Anatomy']],
                            ['Biochemistry', ['Metabolism', 'Molecular Biology', 'Lab Techniques', 'Protein Structure']],
                            ['Physiology', ['Organ Systems', 'Homeostasis', 'Endocrine', 'Immune System']],
                            ['Molecular Biology', ['DNA/RNA', 'Gene Expression', 'Biotechnology', 'Cell Signaling']]
                        ],
                        'weight': 0.25
                    },
                    'Psych/Soc': {
                        'topics': [
                            ['Psychology', ['Learning', 'Memory', 'Cognition', 'Development', 'Personality']],
                            ['Sociology', ['Social Structure', 'Demographics', 'Inequality', 'Social Change']],
                            ['Social Psychology', ['Attitudes', 'Group Dynamics', 'Attribution', 'Prejudice']],
                            ['Research Methods', ['Study Design', 'Statistics', 'Ethics', 'Validity']]
                        ],
                        'weight': 0.25
                    }
                },
                'num_choices': 4,
                'time_per_q': 95,
                'guessing_param': 0.25
            }
        }
    
    def generate_irt_parameters(self, difficulty_label: str) -> Tuple[float, float, float]:
        """Generate IRT parameters based on difficulty label"""
        if difficulty_label == 'easy':
            a = np.random.uniform(0.7, 1.1)  # Discrimination
            b = np.random.uniform(-2.5, -0.8)  # Difficulty
        elif difficulty_label == 'medium':
            a = np.random.uniform(0.9, 1.3)
            b = np.random.uniform(-0.7, 0.7)
        else:  # hard
            a = np.random.uniform(1.0, 1.6)
            b = np.random.uniform(0.8, 2.5)
        
        # Round to 2 decimal places
        return round(a, 2), round(b, 2)
    
    def generate_question_stem(self, exam: str, section: str, topic: str, subtopic: str, 
                               difficulty: str) -> Dict[str, Any]:
        """Generate a question stem based on exam, section, topic, and difficulty"""
        
        # Create question templates based on exam and topic
        templates = self._get_question_templates(exam, section, topic, subtopic, difficulty)
        
        # Select a random template
        template = random.choice(templates)
        
        return template
    
    def _get_question_templates(self, exam: str, section: str, topic: str, 
                                subtopic: str, difficulty: str) -> List[Dict]:
        """Get question templates for specific exam/section/topic combination"""
        
        # This is a simplified template system - in production, you'd have thousands of templates
        templates = []
        
        if exam == 'SAT' and section == 'Math':
            if topic == 'Algebra':
                if subtopic == 'Linear Equations':
                    templates.extend([
                        {
                            'stem': f"If {random.randint(2,9)}x + {random.randint(1,9)} = {random.randint(10,30)}, what is the value of x?",
                            'concept': 'solving linear equations'
                        },
                        {
                            'stem': f"The equation {random.randint(2,5)}x - {random.randint(1,9)} = {random.randint(2,5)}x + {random.randint(1,9)} has how many solutions?",
                            'concept': 'equation solutions'
                        }
                    ])
                elif subtopic == 'Systems':
                    templates.extend([
                        {
                            'stem': f"In the system: {random.randint(2,5)}x + y = {random.randint(5,15)} and x - y = {random.randint(1,5)}, what is x + y?",
                            'concept': 'systems of equations'
                        }
                    ])
        
        elif exam == 'GRE' and section == 'Quant':
            if topic == 'Data Analysis':
                if subtopic == 'Probability':
                    templates.extend([
                        {
                            'stem': f"A bag contains {random.randint(3,7)} red and {random.randint(2,6)} blue marbles. If two marbles are drawn without replacement, what is the probability both are red?",
                            'concept': 'probability without replacement'
                        }
                    ])
        
        elif exam == 'LSAT' and section == 'Logical Reasoning':
            if topic == 'Arguments':
                templates.extend([
                    {
                        'stem': "The argument's conclusion follows logically if which of the following is assumed?",
                        'concept': 'necessary assumption'
                    },
                    {
                        'stem': "Which of the following, if true, most strengthens the argument?",
                        'concept': 'strengthen argument'
                    }
                ])
        
        elif exam == 'MCAT' and section == 'Chem/Phys':
            if topic == 'Physics':
                templates.extend([
                    {
                        'stem': f"A {random.randint(2,10)} kg object accelerates at {random.randint(2,8)} m/sÂ². What is the net force?",
                        'concept': "Newton's second law"
                    }
                ])
        
        # Default template if no specific ones exist
        if not templates:
            templates = [{
                'stem': f"Question about {subtopic} in {topic}",
                'concept': f'{subtopic.lower()}'
            }]
        
        return templates
    
    def generate_choices(self, exam: str, correct_value: Any, num_choices: int) -> Tuple[List[str], int]:
        """Generate multiple choice options with one correct answer"""
        choices = []
        correct_index = random.randint(0, num_choices - 1)
        
        for i in range(num_choices):
            if i == correct_index:
                choices.append(str(correct_value))
            else:
                # Generate plausible distractors
                if isinstance(correct_value, (int, float)):
                    # Numerical distractors
                    distractor = correct_value + random.choice([-2, -1, 1, 2]) * random.uniform(0.5, 2)
                    choices.append(str(round(distractor, 2)))
                else:
                    # Text distractors
                    choices.append(f"Distractor option {i+1}")
        
        return choices, correct_index
    
    def generate_single_question(self, exam: str, section: str, topic_path: List, 
                                difficulty_label: str, question_num: int) -> Dict:
        """Generate a single complete question with all metadata"""
        
        config = self.exam_configs[exam]
        
        # Generate IRT parameters
        a, b = self.generate_irt_parameters(difficulty_label)
        c = config['guessing_param']
        
        # Generate question content
        topic, subtopic = topic_path[0], topic_path[1]
        question_template = self.generate_question_stem(exam, section, topic, subtopic, difficulty_label)
        
        # Generate choices
        correct_value = random.randint(1, 100)  # Simplified - would be calculated based on stem
        choices, correct_index = self.generate_choices(exam, correct_value, config['num_choices'])
        
        # Generate question ID
        question_id = f"{exam.lower()}-{section[:4].lower()}-{question_num:04d}"
        
        # Create full question object
        question = {
            "id": question_id,
            "exam": exam,
            "section": section,
            "topic_path": topic_path,
            "stem": question_template['stem'],
            "choices": choices,
            "correct_index": correct_index,
            "explanation": f"The correct answer is {choices[correct_index]}. This question tests {question_template['concept']}.",
            "difficulty_label": difficulty_label,
            "irt": {
                "a": a,
                "b": b, 
                "c": c
            },
            "time_sec_suggested": config['time_per_q'],
            "tags": [topic.lower().replace(' ', '-'), subtopic.lower().replace(' ', '-'), question_template['concept'].replace(' ', '-')],
            "source_note": "generated-v1",
            "needs_review": False,
            "created_at": datetime.utcnow().isoformat()
        }
        
        return question
    
    def generate_exam_questions(self, exam: str, total_questions: int = 3000) -> List[Dict]:
        """Generate all questions for a single exam"""
        print(f"\nðŸŽ¯ Generating {total_questions} questions for {exam}...")
        
        questions = []
        config = self.exam_configs[exam]
        
        # Calculate questions per section based on weights
        section_distribution = {}
        for section, section_config in config['sections'].items():
            section_distribution[section] = int(total_questions * section_config['weight'])
        
        # Adjust for rounding
        total_allocated = sum(section_distribution.values())
        if total_allocated < total_questions:
            # Add remaining to largest section
            largest_section = max(section_distribution, key=section_distribution.get)
            section_distribution[largest_section] += (total_questions - total_allocated)
        
        question_counter = 1
        
        # Generate questions for each section
        for section, num_questions in section_distribution.items():
            print(f"  ðŸ“š Generating {num_questions} questions for {section}...")
            
            section_config = config['sections'][section]
            topics = section_config['topics']
            
            # Difficulty distribution: 30% easy, 50% medium, 20% hard
            difficulty_distribution = {
                'easy': int(num_questions * 0.3),
                'medium': int(num_questions * 0.5),
                'hard': int(num_questions * 0.2)
            }
            
            # Adjust for rounding
            total_diff = sum(difficulty_distribution.values())
            if total_diff < num_questions:
                difficulty_distribution['medium'] += (num_questions - total_diff)
            
            # Generate questions for each difficulty level
            for difficulty, count in difficulty_distribution.items():
                for i in range(count):
                    # Select random topic and subtopic
                    topic_group = random.choice(topics)
                    topic = topic_group[0]
                    subtopic = random.choice(topic_group[1])
                    
                    question = self.generate_single_question(
                        exam=exam,
                        section=section,
                        topic_path=[topic, subtopic],
                        difficulty_label=difficulty,
                        question_num=question_counter
                    )
                    
                    questions.append(question)
                    question_counter += 1
                    
                    # Update stats
                    self.stats[exam]['total'] += 1
                    if section not in self.stats[exam]['by_section']:
                        self.stats[exam]['by_section'][section] = 0
                    self.stats[exam]['by_section'][section] += 1
                    
                    if difficulty not in self.stats[exam]['by_difficulty']:
                        self.stats[exam]['by_difficulty'][difficulty] = 0
                    self.stats[exam]['by_difficulty'][difficulty] += 1
        
        print(f"  âœ… Generated {len(questions)} questions for {exam}")
        return questions
    
    def save_questions(self, questions: List[Dict], exam: str, batch_size: int = 500):
        """Save questions to JSON files in batches"""
        import os
        
        output_dir = f"/home/claude/qbank/{exam.lower()}"
        os.makedirs(output_dir, exist_ok=True)
        
        # Save in batches to avoid huge files
        for i in range(0, len(questions), batch_size):
            batch = questions[i:i+batch_size]
            batch_num = (i // batch_size) + 1
            filename = f"{output_dir}/{exam.lower()}_batch_{batch_num:03d}.json"
            
            with open(filename, 'w') as f:
                json.dump(batch, f, indent=2)
            
            print(f"  ðŸ’¾ Saved batch {batch_num} ({len(batch)} questions) to {filename}")
    
    def generate_all_exams(self):
        """Generate questions for all exams"""
        print("="*60)
        print("ðŸš€ EUREKA MASSIVE QUESTION BANK GENERATOR")
        print("="*60)
        
        for exam in ['SAT', 'GRE', 'LSAT', 'MCAT']:
            questions = self.generate_exam_questions(exam, total_questions=3000)
            self.generated_questions[exam] = questions
            self.save_questions(questions, exam)
        
        self.print_summary()
    
    def print_summary(self):
        """Print generation summary"""
        print("\n" + "="*60)
        print("ðŸ“Š GENERATION SUMMARY")
        print("="*60)
        
        total_all_exams = 0
        
        for exam in ['SAT', 'GRE', 'LSAT', 'MCAT']:
            stats = self.stats[exam]
            print(f"\n{exam}:")
            print(f"  Total: {stats['total']}")
            print(f"  By Section:")
            for section, count in stats['by_section'].items():
                print(f"    {section}: {count}")
            print(f"  By Difficulty:")
            for diff, count in stats['by_difficulty'].items():
                print(f"    {diff}: {count}")
            
            total_all_exams += stats['total']
        
        print(f"\nðŸŽ‰ TOTAL QUESTIONS GENERATED: {total_all_exams}")
        print("="*60)


def main():
    """Main generation function"""
    generator = MassiveQBankGenerator()
    generator.generate_all_exams()


if __name__ == "__main__":
    main()
